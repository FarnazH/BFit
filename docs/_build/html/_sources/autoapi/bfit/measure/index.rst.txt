:py:mod:`bfit.measure`
======================

.. py:module:: bfit.measure

.. autoapi-nested-parse::

   Deviation Measure Module.

   Classes that measure the deviation between true and model probability distributions.

   Classes
   -------
       KLDivergence - Kullback-Leibler divergence measure.
       SquaredDifference - Square-Difference measure for Least-Squared method.

   ..
       !! processed by numpydoc !!


Module Contents
---------------

Classes
~~~~~~~

.. autoapisummary::

   bfit.measure.SquaredDifference
   bfit.measure.KLDivergence




.. py:class:: SquaredDifference(density)

   
   Squared Difference Class for performing the Least-Squared method.

   Least-squared is the defined to be the integral of the squared difference between
    true and model functions:
   .. math::
       ||f - g|| = \int_G (f(x) - g(x))^2 dx
   where,
       :math:`f` is the true function,
       :math:`g` is the model function,
       :math:`G` is the grid.

   The term :math:`|f(x) - g(x)|^2` is called the Squared Difference between f and g on a point x.

   .. attribute:: density

      The true function (being approximated) evaluated on `N` points.

      :type: ndarray(N,)

   .. method:: evaluate(deriv=False)

      Return the squared difference between model and true functions. Note that it does not
      integrate the model but rather only computes f(x) - g(x). If `deriv` is True,
      then the derivative with respect to model function is returned.

   .. rubric:: Notes

   - This class does not return the Least-Squared but rather the squared difference.
       One would need to integrate this to get the Least Squared.















   ..
       !! processed by numpydoc !!
   .. py:method:: evaluate(self, model, deriv=False)

      
      Evaluate squared difference b/w density & model on the grid points.

      This is defined to be :math:`(f(x) - g(x))^2`.

      :param model: The model density evaluated on the grid points.
      :type model: ndarray, (N,)
      :param deriv: Whether to compute the derivative of squared difference w.r.t. model density.
      :type deriv: bool, optional

      :returns: * **m** (*ndarray, (N,)*) -- The squared difference between density & model on the grid points.
                * **dm** (*ndarray, (N,)*) -- The derivative of squared difference w.r.t. model density evaluated on the
                  grid points. Only returned if `deriv=True`.















      ..
          !! processed by numpydoc !!


.. py:class:: KLDivergence(density, mask_value=1e-12)

   
   Kullback-Leibler Divergence Class.

   This is defined as the integral:
   .. math::
       D(f, g) := \int_G f(x) \ln ( \frac{f(x)}{g(x)} ) dx
   where,
       :math:`f` is the true probability distribution,
       :math:`g` is the model probability distribution,
       :math:`G` is the grid.

   .. attribute:: density

      The true function (being approximated) evaluated on `N` points.

      :type: ndarray(N,)

   .. attribute:: mask_value

      Values of model density `g` that are less than `mask_value` are masked when used in division
       and then replaced with the value of 1 so that logarithm of one is zero.

      :type: float

   .. method:: evaluate(deriv=False)

      Return the integrand :math:`f(x) \ln(f(x)/g(x))` between model and true functions.
      Note that it does not integrate the model, and so it doesn't exactly return the
      Kullback-Leibler. If `deriv` is True, then the derivative with respect to model function
      is returned.

   .. rubric:: Notes

   - This class does not return the Kullback-Leibler but rather the integrand.
       One would need to integrate this to get the Least Squared.
   - It using masked values to handle overflow and underflow floating point precision issues. This
       is due to the division in the Kullback-Leibler formula between two probability
       distributions.















   ..
       !! processed by numpydoc !!
   .. py:method:: evaluate(self, model, deriv=False)

      
      Evaluate the integrand of Kullback-Leibler divergence b/w true & model.

      .. math ::
          D(f, g) := \int_G f(x) \ln ( \frac{f(x)}{g(x)} ) dx
      where,
          :math:`f` is the true probability distribution,
          :math:`g` is the model probability distribution,
          :math:`G` is the grid.

      If the model density is negative, then this function will return extremely large values,
      for optimization purposes.

      :param model: The model density evaluated on the grid points.
      :type model: ndarray, (N,)
      :param deriv: Whether to compute the derivative of divergence w.r.t. model density.
      :type deriv: bool, optional

      :returns: * **m** (*ndarray, (N,)*) -- The divergence between density & model on the grid points.
                * **dm** (*ndarray, (N,)*) -- The derivative of divergence w.r.t. model density evaluated on the grid points.
                  Only returned if `deriv=True`.

      .. rubric:: Notes

      - Values of Model density that are less than `mask_value` are masked when used in
          division and then replaced with the value of 1 so that logarithm of one is zero.















      ..
          !! processed by numpydoc !!


