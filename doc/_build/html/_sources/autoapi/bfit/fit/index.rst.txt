:py:mod:`bfit.fit`
==================

.. py:module:: bfit.fit

.. autoapi-nested-parse::

   Fitting Algorithms Module.

   Contains the optimization algorithms for fitting a linear combination of Gaussian functions.

   Classes
   -------
       KLDivergenceSCF : Optimizes coefficients and exponents using self-consistent method.
       GaussianBasisFit : Optimizes coefficients and exponents using `Scipy.optimize` function.


   .. rubric:: Examples

   The goal is to fit a Gaussian density to some function.
   def f(x) :
       # Insert what it does here.
       return ...

   The first step is to define the grid object.
   >> from bfit.grid import CubicGrid
   >> grid = CubicGrid(0.01, 0.99, 0.01)

   Place the values of `f` on those grid points in an array.
   >> density = f(grid.points)

   Define the model, that you want to fit with.
   >> from bfit.model import AtomicGaussianDensity, MolecularGaussianDensity
   >> model = AtomicGaussianDensity(grid.points, num_s=5, num_p=5, normalize=True)
   >> # model = MolecularGaussianDensity(grid.points, np.array([[0., 0., 0.], [1., 1., 1.]]),
                                       # np.array([[5, 5], [5, 5]]))

   Define which algorithm you want to optmize.
   >> fit = KLDivergenceSCF(grid, density, model)

   Optimize the coefficients and exponents but give an initial guess.
   >> initc = [1.] * 10
   >> inite = np.array([0.001, 0.01, 0.1, 1., 2., 5., 10., 50., 75., 100.])
   >> result = fit.run(initc, inite)

   Print out the results.
   >> print("Optimized coefficients are: ", result["x"][0])
   >> print("Optimized exponents are: ", result["x"][1])
   >> print("Final performance measures are: ", result["performance"][-1])
   >> print("Was it successful? ", result["success"])

   .. rubric:: Notes

   - The algorithm uses masked value for floating point precision for KLDivergenceSCF.
       This is due to the division found in the Kullback-Leibler formula. It is recommended to use
       `np.float64` or `np.float128` when storing the arrays. A higher Mask value will work as well
       but may cause poor precision. Alternatively, a well-chosen grid and/or initial guesses will
       avoid overflow/underflow floating-point issues.

   .. rubric:: References

   [1] BFit: Information-Theoretic Approach to Basis-Set Fitting of Electron Densities
               Alireza Tehrani, Farnaz Heidar-Zadeh, James S. M. Anderson, Toon Verstraelen, and
                TODO Add more authors if needed ... Paul W. Ayers.

   ..
       !! processed by numpydoc !!


Module Contents
---------------

Classes
~~~~~~~

.. autoapisummary::

   bfit.fit.KLDivergenceSCF
   bfit.fit.GaussianBasisFit




.. py:class:: KLDivergenceSCF(grid, density, model, mask_value=0.0, integration_val=None)

   Bases: :py:obj:`_BaseFit`

   
   Kullback-Leiber Divergence Self-Consistent Fitting.

   Optimizes the coefficients and exponents of the Gaussian Model to a function using the
   Kullback-Leibler divergence measure with the constraint that it is normalized.

   .. attribute:: grid

      Grid class that contains the grid points and integration methods on them.

      :type: (_BaseRadialGrid, CubicGrid)

   .. attribute:: density

      The true function evaluated on the grid points from `grid`.

      :type: ndarray(N,)

   .. attribute:: model

      The Gaussian basis model density. Located in `model.py`.

      :type: (AtomicGaussianDensity, MolecularGaussianDensity)

   .. attribute:: measure

      The deviation measure between true density and model density. Located in `measure.py`

      :type: (SquaredDifference, KLDivergence)

   .. attribute:: norm

      The integral of the attribute `density`.

      :type: float

   .. attribute:: lagrange_multiplier

      Lagrange multiplier.

      :type: float

   .. method:: run(): dict

      Runs the optimizing algorithm for optimizing coefficients and exponents to a linear
      combination of Gaussian functions.

   .. rubric:: Examples

   The goal is to fit a Gaussian density to some function.
   def f(x) :
       # Insert what it does here.
       return ...

   The first step is to define the grid object.
   >> grid = CubicGrid(0.01, 0.99, 0.01)

   Place the values of `f` on those grid points in an array.
   >> density = f(grid.points)

   Define the model, that you want to fit with.
   >> model = AtomicGaussianDensity(grid.points, num_s=5, num_p=5, normalize=True)

   Define which algorithm you want to optmize.
   >> fit = KLDivergenceSCF(grid, density, model)

   Optimize the coefficients and exponents but give an initial guess.
   >> initc = [1.] * 10
   >> inite = np.array([0.001, 0.01, 0.1, 1., 2., 5., 10., 50., 75., 100.])
   >> result = fit.run(initc, inite)

   Print out the results.
   >> print("Optimized coefficients are: ", result["x"][0])
   >> print("Optimized exponents are: ", result["x"][1])
   >> print("Final performance measures are: ", result["performance"][-1])
   >> print("Was it successful? ", result["success"])

   .. rubric:: Notes

   - The algorithm uses masked value for floating point precision. This is due to the division found
   in the Kullback-Leibler formula. It is recommended to use `np.float64` or `np.float128` when
   storing the arrays. A higher Mask value will work as well but may cause poor precision.
   Alternatively, a well-chosen grid and/or initial guesses will avoid overflow/underflow
   floating-point issues.

   .. rubric:: References

   [1] BFit: Information-Theoretic Approach to Basis-Set Fitting of Electron Densities
           Alireza Tehrani, Farnaz Heidar-Zadeh, James S. M. Anderson, Toon Verstraelen, and
            TODO Add more authors if needed ... Paul W. Ayers.















   ..
       !! processed by numpydoc !!
   .. py:method:: lagrange_multiplier(self)
      :property:

      
      Obtain the lagrange multiplier.
















      ..
          !! processed by numpydoc !!

   .. py:method:: _update_params(self, coeffs, expons, update_coeffs=True, update_expons=False)

      
      Compute updated coefficients & exponents of Gaussian basis functions.

      :param coeffs: The initial coefficients of Gaussian basis functions.
      :type coeffs: ndarray
      :param expons: The initial exponents of Gaussian basis functions.
      :type expons: ndarray
      :param update_coeffs: Whether to optimize coefficients of Gaussian basis functions.
      :type update_coeffs: bool, optional
      :param update_expons: Whether to optimize exponents of Gaussian basis functions.
      :type update_expons: bool, optional

      :returns: * **coeffs** (*ndarray*) -- The updated coefficients of Gaussian basis functions. Only returned if `deriv=True`.
                * **expons** (*ndarray*) -- The updated exponents of Gaussian basis functions. Only returned if `deriv=True`.















      ..
          !! processed by numpydoc !!

   .. py:method:: run(self, c0, e0, opt_coeffs=True, opt_expons=True, maxiter=500, c_threshold=1e-06, e_threshold=1e-06, d_threshold=1e-06, disp=False)

      
      Optimize the coefficients & exponents of Gaussian basis functions self-consistently.

      :param c0: The initial coefficients of Gaussian basis functions.
      :type c0: ndarray
      :param e0: The initial exponents of Gaussian basis functions.
      :type e0: ndarray
      :param opt_coeffs: Whether to optimize coefficients of Gaussian basis functions.
      :type opt_coeffs: bool, optional
      :param opt_expons: Whether to optimize exponents of Gaussian basis functions.
      :type opt_expons: bool, optional
      :param maxiter: Maximum number of iterations.
      :type maxiter: int, optional
      :param c_threshold: The convergence threshold for absolute change in coefficients. Default is 1e-6.
      :type c_threshold: float
      :param e_threshold: The convergence threshold for absolute change in exponents. Default is 1e-6.
      :type e_threshold: float
      :param d_threshold: The convergence threshold for absolute change in divergence value. Default is 1e-6.
      :type d_threshold: float
      :param disp: If true, then at each iteration the integral, :math:`L_1`, :math:`L_\infty` and
                   Kullback-Leibler measure is printed. Default is False.
      :type disp: bool

      :returns: **result** -- The optimization results presented as a dictionary containing:
                "x" : (ndarray, ndarray)
                    The optimized coefficients and exponents.
                "success": bool
                    Whether or not the optimization exited successfully.
                "fun" : ndarray
                    Values of KL divergence (objective function) at each iteration.
                "performance" : ndarray
                    Values of various performance measures of modeled density at each iteration,
                    as computed by `goodness_of_fit()` method.
                "time" : float
                    The time in seconds it took to complete the algorithm.
      :rtype: dict















      ..
          !! processed by numpydoc !!


.. py:class:: GaussianBasisFit(grid, density, model, measure='KL', method='SLSQP', weights=None, mask_value=1e-10)

   Bases: :py:obj:`_BaseFit`

   
   Optimizes either least-squares or Kullback-Leibler of Gaussian funcs using `Scipy.optimize`.

   The Gaussian functions can be constrained to have their integral be a fixed value.
       Although it is not recommended. The coefficients and exponents are always bounded to be
        positive.

   .. attribute:: grid

      Grid class that contains the grid points and integration methods on them.

      :type: (_BaseRadialGrid, CubicGrid)

   .. attribute:: density

      The true function evaluated on the grid points from `grid`.

      :type: ndarray(N,)

   .. attribute:: model

      The Gaussian basis model density. Located in `model.py`.

      :type: (AtomicGaussianDensity, MolecularGaussianDensity)

   .. attribute:: measure

      The deviation measure between true density and model density that is minimized.
      Can be either be "KL" (Kullback-Leibler, default) or "LS" (least-squares).

      :type: str, optional

   .. attribute:: norm

      The integration of the density over the grid.

      :type: float

   .. method:: run() :

   .. method:: func() :

   .. method:: evaluate_model() :

   .. rubric:: Examples

   The goal is to fit a Gaussian density to some function.
   def f(x) :
       # Insert what it does here.
       return ...

   The first step is to define the grid object.
   >> grid = CubicGrid(0.01, 0.99, 0.01)

   Place the values of `f` on those grid points in an array.
   >> density = f(grid.points)

   Define the model, that you want to fit with.
   >> model = AtomicGaussianDensity(grid.points, num_s=5, num_p=5, normalize=True)

   Define which measure (least-squares) and "scipy.optimize" algorithm to use.
   >> fit = GaussianBasisFit(grid, density, model, measure="LS", method="SLSQP")

   Optimize the coefficients and exponents but give an initial guess.
   >> initc = [1.] * 10
   >> inite = np.array([0.001, 0.01, 0.1, 1., 2., 5., 10., 50., 75., 100.])
   >> result = fit.run(initc, inite)

   Print out the results.
   >> print("Optimized coefficients are: ", result["x"][0])
   >> print("Optimized exponents are: ", result["x"][1])
   >> print("Final performance measures are: ", result["performance"])
   >> print("Was it successful? ", result["success"])
   >> print("Why it terminated? ", result["message"])

   .. rubric:: Notes

   - The coefficients and exponents are bounded to be positive.

   - These methods in this class was found to be extremely hard to optimize. There appears
       to have many local minimas and Quasi-Newton methods seems inadequate in order to optimize
       these. Just the mere act of placing the initial guess to be close to the solution causes
       problems. It is highly recommended to have `with_constraint` to be False.

   - Note that the Kullback-Leibler between two functions f and g is positive if and only if
       the integrals of f and g are identical.  This constraint must be added for
       these optimizers.















   ..
       !! processed by numpydoc !!
   .. py:method:: run(self, c0, e0, opt_coeffs=True, opt_expons=True, maxiter=1000, tol=1e-14, disp=False, with_constraint=True)

      
      Optimize coefficients and/or exponents of Gaussian basis functions with constraint.

      :param c0: Initial guess for coefficients of Gaussian basis functions.
      :type c0: ndarray
      :param e0: Initial guess for exponents of Gaussian basis functions.
      :type e0: ndarray
      :param opt_coeffs: Whether to optimize coefficients of Gaussian basis functions.
      :type opt_coeffs: bool, optional
      :param opt_expons: Whether to optimize exponents of Gaussian basis functions.
      :type opt_expons: bool, optional
      :param maxiter: Maximum number of iterations.
      :type maxiter: int, optional
      :param tol: For slsqp. precision goal for the value of objective function in the stopping criterion.
                  For trust-constr, it is precision goal for the change in independent variables.
      :type tol: float, optional
      :param disp: If True, then it will print the convergence messages from the optimizer.
      :type disp: bool
      :param with_constraint: If true, then adds the constraint that the integration of the model density must
                              be equal to the constraint of true density. The default is True.
      :type with_constraint: bool

      :returns: **result** -- The optimization results presented as a dictionary containing:
                "x" : (ndarray, ndarray)
                    The optimized coefficients and exponents, respectively.
                "success": bool
                    Whether or not the optimization exited successfully.
                "message" : str
                    Message about the cause of termination.
                "fun" : float
                    Values of KL divergence (objective function) at the final iteration.
                "jacobian": ndarray
                    The Jacobian of the coefficients and exponents.
                "performance" : list
                    Values of various performance measures of modeled density at each iteration,
                    as computed by `_BaseFit.goodness_of_fit` method.
                "time" : float
                    The time in seconds it took to optimize.
      :rtype: dict

      .. rubric:: Notes

      - This is a constrained optimization such that the integration of the model density is
          a fixed value. Hence, only certain optimization algorithms can be used.
      - The coefficients and exponents are bounded to be positive.















      ..
          !! processed by numpydoc !!

   .. py:method:: func(self, x, *args)

      
      Compute objective function and its derivative w.r.t. Gaussian basis parameters.

      :param x: The parameters of Gaussian basis which is being optimized. Contains both the
                coefficients and exponents together in a 1-D array.
      :type x: ndarray
      :param args: Additional arguments to the model.

      :returns: The objective function value and its derivative wrt to coefficients and exponents.
      :rtype: float, ndarray















      ..
          !! processed by numpydoc !!

   .. py:method:: const_norm(self, x, *args)

      
      Compute deviation in normalization constraint.

      :param x: The parameters of Gaussian basis which is being optimized. Contains both the
                coefficients and exponents together in a 1-D array.
      :type x: ndarray
      :param args: Additional parameters for the model.

      :returns: The deviation of the integrla with the normalization constant.
      :rtype: float















      ..
          !! processed by numpydoc !!

   .. py:method:: evaluate_model(self, x, *args)

      
      Evaluate the model density & its derivative.

      :param x: The parameters of Gaussian basis which is being optimized. Contains both the
                coefficients and exponents together in a 1-D array.
      :type x: ndarray
      :param args: Additional parameters for the model.

      :returns: Evaluates the model density & its derivative.
      :rtype: float, ndarray















      ..
          !! processed by numpydoc !!


